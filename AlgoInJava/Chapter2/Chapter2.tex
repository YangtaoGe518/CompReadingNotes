\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{comment}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{amsthm}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newtheorem{theorem}{Theorem}[section]

\DeclarePairedDelimiter \abs{\lvert}{\rvert} % short cut for absolute value

\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

\title{Chapter 2: Algorithm Analysis}
\author{Yangtao Ge}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
This section discusses about:
\begin{itemize}
    \item how to estimate the time required for a program
    \item how to reduce running time of a program
    \item The result of careless use of recursion
    \item very efficient algorithms to raise a number to a power
    \item compute GCD
\end{itemize}
\end{abstract}
\section{Mathematical Background}
Four definitions of the framework:
\begin{definition}[Upper bound]
    $T(N) = O(f(N))$ if there are positive \emph{constants} $c$ and $n_0$ such that $T(N) \leq cf(N)$
    when $N \geq n_0$
\end{definition}

\begin{definition}[Lower bound]
    $T(N) = \Omega(g(N))$ if there are positive \emph{constants} $c$ and $n_0$ such that $T(N) \geq cg(N)$
    when $N \geq n_0$
\end{definition}

\begin{definition}[Envelope]
    $T(N) = \Theta(h(N))$ iff $T(N) - O(h(N))$ and $T(N) = \Omega(h(N))$
\end{definition}

\begin{definition}
    $T(N) = o(p(N))$ if for \textbf{\textit{all}} positive constants $c$ there exists an $n_0$ such that
    $T(N) < cp(N)$ when $N > n_0$.
\end{definition}
\emph{Ref: p.30 for detail theorem}

Typical growth rates:\newline
\begin{tabular}{|p{3cm}|p{5cm}|}
    \hline
    Function & Name\\
    \hline
    \hline
    $c$ & Constant \\
    $\log_{}N$ & Logarithmic \\
    $\log^2_{}N$ & Log-Squared \\
    $N$ & Linear \\
    $N\log_{}N$ & \ \\ 
    $N^2$ & Quadratic \\
    $N^3$ & Cubic \\
    $2^N$ & Exponential \\ 
    \hline
\end{tabular}

Some Theorem from the definition:
\begin{theorem}
    If $T_1(N) = O(f(N))$ and $T_2(N) = O(g(N))$, then:
    \begin{enumerate}
        \item $T_1(N) + T_2(N) = O(f(N) + g(N))$
        \item $T_1(N) * T_2(N) = O(f(N) * g(N))$
    \end{enumerate}
\end{theorem}

\begin{theorem}
    If $T(N$) is a polynomial of degree $k$, then $T(N) = \Theta(N^k)$
\end{theorem}

\begin{theorem}
    $\log^k_{}N = O(N)$ for any constant $k$, which tell us `Logarithms grow very slowly
\end{theorem}

For big-O notation answers: \underline{Lower-order terms can generally be ignored}
e.g. $f(N) = 2N^2 + N$ then its big-O notation is $T(N) = O(N^2)$

For \emph{relative growth rates} of two Function, we using `\emph{L'Hopitals's rule}' to determine it:
\begin{theorem}[L'Hopitals's rule]
    If $\lim_{N\to\infty}f(N) = \infty$ and $\lim_{N\to\infty}g(N) = \infty$, 
    then $\lim_{N\to\infty} \frac{f(N)}{g(N)} = \lim_{N\to\infty} \frac{f'(N)}{g'(N)}$
\end{theorem} 

Four possible results:
\begin{itemize}
    \item Limit is 0: $f(N) = 0(g(N))$
    \item Limit is $c\neq 0$: $f(N) = \Theta(g(N))$
    \item Limit is $\infty$: $g(N) = o(f(N))$
    \item Limit does not exist: No relations
\end{itemize}

\section{Model}
Basically a normal computer which execute instructions sequentially. And it has inifinite memory

\section{What to Analyze}
What we focus:
\begin{itemize}
    \item Input size
    \item Running time
\end{itemize}

What we use:
\begin{itemize}
    \item $T_{avg}(N)$ : average running time -- reflect typical behaviour.
    \item $T_{worst}(N)$: worst running time -- a guarantee for performance on any possible input
\end{itemize}
\emph{Ref: pp. 33 - 35} (Maximum subsequence Sum Problem)

\section{Running Time Calculations}
We only focus on \emph{big-O notation} answers.

\subsection{A simple Example}
Calculations of $\sum_{i=1}^N i^3$. The code is as follows:
\begin{lstlisting}[language=Java]
    public static int sum(int n){
        int partialSum = 0;           // Line 1
        for(int i = 1; i <= n; i++){  // Line 2
            partialSum += i * i * i;  // Line 3
        }
        return partialSum;            // Line 4
    }
\end{lstlisting}

\underline{Analysis:}
\begin{itemize}
    \item Line 1: 1 time for assignment
    \item Line 2: 2N + 2 times in total:
    \begin{itemize}
        \item 1 time for initialization
        \item N + 1 times for comparison
        \item N times for increment
    \end{itemize}
    \item Line 3: 4N times in total(2 multiplication, 1 addition, 1 assignment)
    \item Line 4: 1 time for return
\end{itemize}
In total $6N + 4$, so we say that it is a $O(N)$ method 

\subsection{General Rules}
\begin{enumerate}
    \item `For' loops: $T = T_{statements} * iterations$ (at most)
    \item `Nested' loops: $T = T_{statements} * \prod_{}SizeOfLoop$ (Analysis them inside-out)
    \item Consecutive statements: Use `big-O' notation add \newline 
    e.g. $O(N)$ followed by $O(N^2)$ is still $O(N^2)$
    \item `if-else': $T = \max(T_{Stat1}, T_{Stat2})$ (sometimes overestimate, but never underestimate)
\end{enumerate}

\subsection{Solution for the Problem of Maximum subsequence Sum}
Four algorithms are provided here (\emph{Ref: pp.39 - 49})

\subsection{Logarithms in the Running Time}
Some Logarithms algorithms:
\begin{itemize}
    \item \emph{divide-and-conquer} algorithms: $O(N\log_{}N)$ time 
    \item General rules:
    \begin{itemize}
        \item $O(\log_{}N)$: takes constant $O(1)$ time to cut the problem size by a fraction (usually $1/2$)
        \item $O(N)$: constant time requred to merely reduce the problem by a constant amount
    \end{itemize}
\end{itemize}

Following subsection are some common Logarithmic algorithms:

\subsubsection{Binary Search}

\subsubsection{Euclid's Algorithm}

\subsubsection{Exponentiation}

\subsection{A Grain of Salt}
Sometimes, Worst case is better than average case:
\begin{itemize}
    \item average case is very complex
    \item Analysis needs to be tightened
    \item average running time is less significant than worst case running time
\end{itemize}





\end{document}